{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONV3D - Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project involves building a 3D Convolutional Neural Network (CNN) to correctly recognize hand gestures by a user to control a smart TV.\n",
    "\n",
    "The objective of this projects is to build a hand gesture recognition model that can be hosted on a camera installed in a smart TV that can understand 5 gestures. Namely, leftwards hand movement to go to previous channel, rightward hand movement to go to next channel, upward hand movement to increase the volume, downward hand movement to decrease the volume and a palm gesture to pause playing the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#tf.set_random_seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, folder names for training and validation are read. We also set the batch_size here. We set the batch size in such a way that we are able to use the GPU in full capacity. We keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('C:\\\\Users\\\\adithya\\\\OneDrive\\\\Desktop\\\\dl\\\\Project_data\\\\train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('C:\\\\Users\\\\adithya\\\\OneDrive\\\\Desktop\\\\dl\\\\Project_data\\\\val.csv').readlines())\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONV3D Generator\n",
    "\n",
    "In the generator, we are going to preprocess the images as we have images of 2 different dimensions as well as create a batch of video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        for batch in range(num_batches):\n",
    "            batch_data = np.zeros((batch_size,18,60,60,3))\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imresize(image,(60,60))\n",
    "                    image = image - np.min(image)/ (np.max(image) - np.min(image))\n",
    "                   \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] \n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] \n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] \n",
    "\n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "        if (len(t)%batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,18,60,60,3))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imresize(image,(60,60))\n",
    "                    image = image - np.min(image)/ (np.max(image) - np.min(image))\n",
    "                                      \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] \n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] \n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] \n",
    "\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'C:\\\\Users\\\\adithya\\\\OneDrive\\\\Desktop\\\\dl\\\\Project_data\\\\train'\n",
    "val_path = 'C:\\\\Users\\\\adithya\\\\OneDrive\\\\Desktop\\\\dl\\\\Project_data\\\\val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "A model is created using different functionalities that Keras provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (2,2,2), strides=(1,1,1), padding='same', input_shape=(18,60,60,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, (2,2,2), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(128, (2,2,2), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(256, (2,2,2), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compiling the model and looking at the summary to find out the number of parameters of the model. The number of parameters of the model is important since the model has to be light enough to be hosted on a webcam itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 18, 60, 60, 32)    800       \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 18, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 18, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 9, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 9, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 9, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 9, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 4, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 4, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 4, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 4, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 2, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 2, 7, 7, 256)      262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 2, 7, 7, 256)      1024      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 2, 7, 7, 256)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 3, 3, 256)      0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,560,549\n",
      "Trainable params: 1,559,589\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "optimiser = optimizers.Adam(lr=0.0002)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the train_generator and the val_generator which will be used in .fit_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nn1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model and saving checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  C:\\Users\\adithya\\OneDrive\\Desktop\\dl\\Project_data\\val ; batch size = 64\n",
      "Source path =  C:\\Users\\adithya\\OneDrive\\Desktop\\dl\\Project_data\\train ; batch size = 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nn1\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\envs\\nn1\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "    Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\nn1\\lib\\site-packages\\ipykernel_launcher.py:41: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\nn1\\lib\\site-packages\\ipykernel_launcher.py:46: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "    Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 78s 7s/step - loss: 2.0019 - categorical_accuracy: 0.2941 - val_loss: 1.8085 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-11-0221_12_03.930404/model-00001-2.02692-0.29412-1.80845-0.27000.h5\n",
      "Epoch 2/30\n",
      "11/11 [==============================] - 65s 6s/step - loss: 1.3421 - categorical_accuracy: 0.3982 - val_loss: 2.1317 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-11-0221_12_03.930404/model-00002-1.34874-0.39819-2.13167-0.23000.h5\n",
      "Epoch 3/30\n",
      "11/11 [==============================] - 64s 6s/step - loss: 1.1509 - categorical_accuracy: 0.5038 - val_loss: 2.9973 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-11-0221_12_03.930404/model-00003-1.15176-0.50377-2.99727-0.23000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 4/30\n",
      "11/11 [==============================] - 63s 6s/step - loss: 0.9905 - categorical_accuracy: 0.6048 - val_loss: 2.3965 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-11-0221_12_03.930404/model-00004-0.98569-0.60483-2.39652-0.23000.h5\n",
      "Epoch 5/30\n",
      "11/11 [==============================] - 64s 6s/step - loss: 0.8662 - categorical_accuracy: 0.6139 - val_loss: 2.3938 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-11-0221_12_03.930404/model-00005-0.87214-0.61388-2.39382-0.23000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 6/30\n",
      "11/11 [==============================] - 64s 6s/step - loss: 0.7749 - categorical_accuracy: 0.6863 - val_loss: 2.1710 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-11-0221_12_03.930404/model-00006-0.76665-0.68627-2.17102-0.25000.h5\n",
      "Epoch 7/30\n",
      "11/11 [==============================] - 63s 6s/step - loss: 0.7248 - categorical_accuracy: 0.7345 - val_loss: 1.9445 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-11-0221_12_03.930404/model-00007-0.72582-0.73454-1.94450-0.28000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/30\n",
      "11/11 [==============================] - 63s 6s/step - loss: 0.7002 - categorical_accuracy: 0.7044 - val_loss: 1.6294 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-11-0221_12_03.930404/model-00008-0.70829-0.70437-1.62936-0.34000.h5\n",
      "Epoch 9/30\n",
      "11/11 [==============================] - 62s 6s/step - loss: 0.6814 - categorical_accuracy: 0.7436 - val_loss: 1.1032 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-11-0221_12_03.930404/model-00009-0.68483-0.74359-1.10317-0.40000.h5\n",
      "Epoch 10/30\n",
      "11/11 [==============================] - 57s 5s/step - loss: 0.6239 - categorical_accuracy: 0.7768 - val_loss: 1.1733 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-11-0221_12_03.930404/model-00010-0.61833-0.77677-1.17333-0.49000.h5\n",
      "Epoch 11/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.5802 - categorical_accuracy: 0.7677 - val_loss: 1.0075 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-11-0221_12_03.930404/model-00011-0.58279-0.76772-1.00753-0.56000.h5\n",
      "Epoch 12/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.5497 - categorical_accuracy: 0.7919 - val_loss: 1.0558 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-11-0221_12_03.930404/model-00012-0.55244-0.79186-1.05584-0.59000.h5\n",
      "Epoch 13/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.4926 - categorical_accuracy: 0.8115 - val_loss: 0.9468 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-11-0221_12_03.930404/model-00013-0.49354-0.81146-0.94677-0.65000.h5\n",
      "Epoch 14/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.4975 - categorical_accuracy: 0.8190 - val_loss: 0.9033 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-11-0221_12_03.930404/model-00014-0.48396-0.81900-0.90333-0.65000.h5\n",
      "Epoch 15/30\n",
      "11/11 [==============================] - 57s 5s/step - loss: 0.4706 - categorical_accuracy: 0.8265 - val_loss: 0.8559 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-11-0221_12_03.930404/model-00015-0.47012-0.82655-0.85592-0.70000.h5\n",
      "Epoch 16/30\n",
      "11/11 [==============================] - 57s 5s/step - loss: 0.4871 - categorical_accuracy: 0.8054 - val_loss: 0.6960 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-11-0221_12_03.930404/model-00016-0.49785-0.80543-0.69599-0.76000.h5\n",
      "Epoch 17/30\n",
      "11/11 [==============================] - 57s 5s/step - loss: 0.4615 - categorical_accuracy: 0.8296 - val_loss: 0.5168 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-11-0221_12_03.930404/model-00017-0.45954-0.82956-0.51684-0.76000.h5\n",
      "Epoch 18/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.4498 - categorical_accuracy: 0.8492 - val_loss: 0.5616 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-11-0221_12_03.930404/model-00018-0.44223-0.84917-0.56156-0.79000.h5\n",
      "Epoch 19/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.4326 - categorical_accuracy: 0.8431 - val_loss: 0.6451 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-11-0221_12_03.930404/model-00019-0.43623-0.84314-0.64507-0.78000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 20/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.3851 - categorical_accuracy: 0.8778 - val_loss: 0.5619 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-11-0221_12_03.930404/model-00020-0.38285-0.87783-0.56189-0.81000.h5\n",
      "Epoch 21/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.3766 - categorical_accuracy: 0.8733 - val_loss: 0.5311 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-11-0221_12_03.930404/model-00021-0.36933-0.87330-0.53108-0.81000.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 22/30\n",
      "11/11 [==============================] - 57s 5s/step - loss: 0.3840 - categorical_accuracy: 0.8808 - val_loss: 0.5509 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-11-0221_12_03.930404/model-00022-0.38952-0.88084-0.55095-0.82000.h5\n",
      "Epoch 23/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.3782 - categorical_accuracy: 0.8627 - val_loss: 0.5201 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-11-0221_12_03.930404/model-00023-0.37907-0.86275-0.52011-0.83000.h5\n",
      "Epoch 24/30\n",
      "11/11 [==============================] - 57s 5s/step - loss: 0.3512 - categorical_accuracy: 0.8869 - val_loss: 0.6903 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-11-0221_12_03.930404/model-00024-0.35339-0.88688-0.69034-0.83000.h5\n",
      "Epoch 25/30\n",
      "11/11 [==============================] - 57s 5s/step - loss: 0.3678 - categorical_accuracy: 0.8793 - val_loss: 0.4652 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-11-0221_12_03.930404/model-00025-0.36466-0.87934-0.46519-0.84000.h5\n",
      "Epoch 26/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.3384 - categorical_accuracy: 0.8944 - val_loss: 0.6563 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-11-0221_12_03.930404/model-00026-0.32700-0.89442-0.65632-0.83000.h5\n",
      "Epoch 27/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.3506 - categorical_accuracy: 0.8929 - val_loss: 0.5120 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-11-0221_12_03.930404/model-00027-0.35168-0.89291-0.51200-0.82000.h5\n",
      "Epoch 28/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.3640 - categorical_accuracy: 0.8793 - val_loss: 0.5695 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-11-0221_12_03.930404/model-00028-0.34922-0.87934-0.56953-0.82000.h5\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 58s 5s/step - loss: 0.3510 - categorical_accuracy: 0.8718 - val_loss: 0.6198 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-11-0221_12_03.930404/model-00029-0.35022-0.87179-0.61978-0.84000.h5\n",
      "Epoch 30/30\n",
      "11/11 [==============================] - 58s 5s/step - loss: 0.3329 - categorical_accuracy: 0.8763 - val_loss: 0.6557 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-11-0221_12_03.930404/model-00030-0.33798-0.87632-0.65575-0.84000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x229c2295a58>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy\n",
    "After 30 epochs, the model results in a 'validation categorical accuracy' of 84%. This can be still be improved by fine tuning batch size and number of epochs. But we consider this to be the best model from the different experiments we tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
